{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"./images/msp_header.png\">\n",
    "</p>\n",
    "\n",
    "#### Prof. Dr. -Ing. Gerald Schuller <br> Jupyter Notebook: Renato Profeta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Multirate Signal Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Multirate:** meaning different sampling\n",
    "rates, as from using downsampling or\n",
    "upsampling. In filter banks (or convolutional\n",
    "Neural Networks, e.g. for pattern\n",
    "recognition), we reduce the sampling rate\n",
    "after filtering a signal, which reduces the\n",
    "bandwidth. For reconstruction and obtaining\n",
    "the original sampling rate, we need to upsample and filter (for interpolation) the\n",
    "signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where is Multirate Signal Processing used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For instance in coding and compression algorithms, like the:\n",
    "- Modified Discrete Cosine Transform (MDCT) filter bank in audio coding \n",
    "- Discrete Cosine Transform (DCT) in image or video coding\n",
    "- Channel coding (OFDM:Orthogonal frequency-division multiplexing), where a channel is divided into many narrower channels with lower data rates and hence longer symbol duration, to reduce problems with multipath/reflections\n",
    "- Machine Learning and Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of a Discrete Time Signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "plt.figure()\n",
    "plt.stem(np.random.randint(-10,10,size=10)/10, use_line_collection=True)\n",
    "plt.title('A Discrete Time Signal')\n",
    "plt.ylabel('x(n)')\n",
    "plt.xlabel('Sample nr. n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Typical sampling rate of audio from a CD:\n",
    "44100 samples/second, or\n",
    "44.1 Khz sampling rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**If you are not familiar with Python, I recommend you to check out some online tutorials**: https://github.com/GuitarsAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python example for a live plot of a microphone signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using Pyaudio, record sound from the audio device and plot, for 8 seconds, and display it live in a Window.\n",
    "Usage example: python pyrecplotanimation.py\n",
    "Gerald Schuller, October 2014 \n",
    "\n",
    "Adapted to Jupyter Notebook - Renato Profeta, August 2019\n",
    "\"\"\"\n",
    "# Imports\n",
    "import pyaudio\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Parameters\n",
    "CHUNK = 1024 #Blocksize\n",
    "WIDTH = 2 #2 bytes per sample\n",
    "CHANNELS = 1 #2\n",
    "RATE = 32000  #Sampling Rate in Hz\n",
    "RECORD_SECONDS = 70\n",
    "\n",
    "# PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=p.get_format_from_width(WIDTH),\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                output=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(0, CHUNK)        # x-array\n",
    "line, = ax.plot(x, 20000.0*np.sin(x))   #Scale axis as this sine function\n",
    "\n",
    "def init():\n",
    "    line.set_ydata(np.ma.array(x, mask=True))\n",
    "    return line,\n",
    "\n",
    "def animate(i):\n",
    "    # update the data\n",
    "    #Reading from audio input stream into data with block length \"CHUNK\":\n",
    "    data = stream.read(CHUNK)\n",
    "    #Convert from stream of bytes to a list of short integers (2 bytes here) in \"samples\":\n",
    "    #shorts = (struct.unpack( \"128h\", data ))\n",
    "    shorts = (struct.unpack( 'h' * CHUNK, data ));\n",
    "    samples=np.array(list(shorts),dtype=float);\n",
    "\n",
    "    #plt.plot(samples)  #<-- here goes the signal processing.\n",
    "    #line.set_ydata(np.log((np.abs(pylab.fft(samples))+0.1))/np.log(10.0))\n",
    "    line.set_ydata(samples)\n",
    "    return line,\n",
    "\n",
    "def handle_close(evt):\n",
    "    # When everything done, release the capture\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    plt.close()\n",
    "plt.connect('close_event', handle_close)\n",
    "\n",
    "# Run animation\n",
    "ani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n",
    "    interval=25, blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Javascript example for a live plot of a microphone signal:\n",
    "Adapted from https://mdn.github.io/voice-change-o-matic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<head>\n",
    "<style>\n",
    ".wrapper {\n",
    "\theight: 100%;\n",
    "\tmax-width: 800px;\n",
    "\tmargin: 0 auto;\n",
    "}\n",
    "header {\n",
    "  height: 120px;\n",
    "}\n",
    "\n",
    "canvas {\n",
    "  border-top: 1px solid black;\n",
    "  border-bottom: 1px solid black;\n",
    "  margin-bottom: -3px;\n",
    "  box-shadow: 0 -2px 4px rgba(0,0,0,0.7),\n",
    "              0 3px 4px rgba(0,0,0,0.7);\n",
    "}\n",
    "\n",
    ".controls {\n",
    "  background-color: rgba(0,0,0,0.1);\n",
    "  height: calc(100% - 225px);\n",
    "}\n",
    "\n",
    "\n",
    "form a {    \n",
    "    background-color: #0088cc;\n",
    "    background-image: linear-gradient(to bottom,  #0088cc 0%,#0055cc 100%); \n",
    "    text-shadow: 1px 1px 1px black;  \n",
    "    text-align: center;\n",
    "    color: white;\n",
    "    border: none;\n",
    "    width: 90\\%;\n",
    "    margin: 1rem auto 0.5rem;\n",
    "    max-width: 80\\%;\n",
    "    font-size: 1.6rem; \n",
    "    line-height: 3rem;\n",
    "    padding: .5rem; \n",
    "    display: block;\n",
    "                    }\n",
    "\n",
    "form a:hover, form a:focus {\n",
    "\tbox-shadow: inset 1px 1px 2px rgba(0,0,0,0.7);\n",
    "}\n",
    "\n",
    "form a:active {\n",
    "\tbox-shadow: inset 2px 2px 3px rgba(0,0,0,0.7);\n",
    "}\n",
    "\n",
    "a#activated {\n",
    "  background-color: #fff;\n",
    "  background-image: linear-gradient(to bottom,  #f00 0%, #a06 100%);\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "a {\n",
    "  color: #aaa;\n",
    "}\n",
    "\n",
    "a:hover, a:focus {\n",
    "  text-decoration: none;\n",
    "}\n",
    "\n",
    "\n",
    " </style>\n",
    "</head>\n",
    "\n",
    "<div class=\"wrapper\">\n",
    "    \n",
    "    <header>\n",
    "      <h1>Audio Waveform</h1>\n",
    "    </header>\n",
    "\n",
    "    <canvas class=\"visualizer\" width=\"640\" height=\"100\"></canvas> \n",
    "<br>\n",
    "    <form class=\"controls\">\n",
    "        <div>\n",
    "            <a class=\"mute\">Mute</a>\n",
    "        </div>\n",
    "    </form>\n",
    "<br>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "// Audio\n",
    "var audioCtx = new (window.AudioContext || window.webkitAudioContext)();\n",
    "var source;\n",
    "var stream;\n",
    "\n",
    "// Button\n",
    "var mute = document.querySelector('.mute');\n",
    "\n",
    "// Audio Nodes\n",
    "var analyser = audioCtx.createAnalyser();\n",
    "analyser.minDecibels = -90;\n",
    "analyser.maxDecibels = -10;\n",
    "analyser.smoothingTimeConstant = 0.85;\n",
    "\n",
    "var gainNode = audioCtx.createGain();\n",
    "\n",
    "// Canvas\n",
    "var canvas = document.querySelector('.visualizer');\n",
    "var canvasCtx = canvas.getContext(\"2d\");\n",
    "var intendedWidth = document.querySelector('.wrapper').clientWidth;\n",
    "canvas.setAttribute('width',intendedWidth);\n",
    "var drawAudio;\n",
    "\n",
    "// Audio Recording\n",
    "if (navigator.mediaDevices.getUserMedia) {\n",
    "     console.log('getUserMedia supported.');\n",
    "     var constraints = {audio: true}\n",
    "     navigator.mediaDevices.getUserMedia (constraints)\n",
    "        .then(\n",
    "          function(stream) {\n",
    "             source = audioCtx.createMediaStreamSource(stream);\n",
    "             source.connect(gainNode);\n",
    "             gainNode.connect(analyser);\n",
    "             //analyser.connect(audioCtx.destination);\n",
    "             visualize();\n",
    "        })\n",
    "        .catch( function(err) { console.log('The following gUM error occured: ' + err);})\n",
    "  } else {\n",
    "     console.log('getUserMedia not supported on your browser!');\n",
    "  }\n",
    "  \n",
    "\n",
    "mute.onclick = voiceMute;\n",
    "\n",
    "function voiceMute() {\n",
    "    if(mute.id === \"\") {\n",
    "      gainNode.gain.setTargetAtTime(0, audioCtx.currentTime, 0)\n",
    "      mute.id = \"activated\";\n",
    "      mute.innerHTML = \"Unmute\";\n",
    "    } else {\n",
    "      gainNode.gain.setTargetAtTime(1, audioCtx.currentTime, 0)\n",
    "      mute.id = \"\";\n",
    "      mute.innerHTML = \"Mute\";\n",
    "    }\n",
    "  }  \n",
    "\n",
    "function visualize() {\n",
    "    var WIDTH = canvas.width;\n",
    "    var HEIGHT = canvas.height;\n",
    "\n",
    "\n",
    "    var visualSetting =\"waveform\";\n",
    "    console.log(visualSetting);\n",
    "\n",
    "    if(visualSetting === \"waveform\") {\n",
    "      analyser.fftSize = 2048;\n",
    "      var bufferLength = analyser.fftSize;\n",
    "      console.log(bufferLength);\n",
    "      var dataArray = new Uint8Array(bufferLength);\n",
    "\n",
    "      canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);\n",
    "\n",
    "      var draw = function() {\n",
    "\n",
    "        drawAudio = requestAnimationFrame(draw);\n",
    "          \n",
    "        analyser.getByteTimeDomainData(dataArray);\n",
    "\n",
    "        canvasCtx.fillStyle = 'rgb(200, 200, 200)';\n",
    "        canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);\n",
    "\n",
    "        canvasCtx.lineWidth = 2;\n",
    "        canvasCtx.strokeStyle = 'rgb(0, 0, 0)';\n",
    "\n",
    "        canvasCtx.beginPath();\n",
    "\n",
    "        var sliceWidth = WIDTH * 1.0 / bufferLength;\n",
    "        var x = 0;\n",
    "\n",
    "        for(var i = 0; i < bufferLength; i++) {\n",
    "\n",
    "          var v = dataArray[i] / 128.0;\n",
    "          var y = v * HEIGHT/2;\n",
    "\n",
    "          if(i === 0) {\n",
    "            canvasCtx.moveTo(x, y);\n",
    "          } else {\n",
    "            canvasCtx.lineTo(x, y);\n",
    "          }\n",
    "\n",
    "          x += sliceWidth;\n",
    "        }\n",
    "\n",
    "        canvasCtx.lineTo(canvas.width, canvas.height/2);\n",
    "        canvasCtx.stroke();\n",
    "      };\n",
    "\n",
    "      draw();\n",
    "\n",
    "\n",
    "    } else if(visualSetting == \"off\") {\n",
    "      canvasCtx.clearRect(0, 0, WIDTH, HEIGHT);\n",
    "      canvasCtx.fillStyle = \"red\";\n",
    "      canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);\n",
    "    }\n",
    "\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nyquist Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The Nyquist theorem tells us: Our signal needs to be **band limited** to **less than half the sampling frequency**. If we have 44.1 kHz sampling, this is less than 22.05 kHz. **Half the sampling frequency** is also called the **Nyquist frequency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- For time discrete signals we only use **normalized frequencies**, normalized to the **sampling frequency** or the **Nyquist frequency**. For the latter, the normalized frequency of 1 would be the Nyquist frequency. Often you also find $\\pi$ as the Nyquist Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Sample Rate Conversion Example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Sampling rate conversion of an audio signal from 44.1 kHz (from a CD) down to 32 kHz on the computer. The signal at 44.1 kHz sampling rate has all frequencies strictly below 22.05 kHz (because of the Nyquist Theorem). A signal at 32 Khz sampling rate needs all frequencies strictly below 16 kHz.\n",
    "- **Observe:** here we lose the highest frequency components (16kHz-22 kHz, which is basically okay since human hearing is usually only up to about 16 kHz). **Before down-sampling we have to remove these high-frequency components by low pass filtering.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Building Blocks of Multirate Signal Processing:\n",
    "### Up-Sampling and Down-Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Upsampling: The other way around, from 32 kHz to 44.1 kHz sampling rate.\n",
    "- **Observe:** here we obtain a new frequency range from 16kHz to 22 kHz, which should contain no signal components. **Here we have to low pass the up-sampled signal to these 16 kHz.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The following picture shows the basic building blocks for low-pass filtering and down-sampling by a factor of N, and upsampling by a factor of N followed by lowpass filtering:\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/upDownSamplingBlocks.PNG\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Observe:** We can do this downsampling and upsampling without loss of information (meaning the reconstructed signal is identical to the lowpass signal), if we obey the **Shannon-Nyquist** law.\n",
    "This means the low pass (LP) needs to be an ideal low pass with  (normalized to the Nyquist frequency at the higher sampling rate)  cutoff frequency $^1$/$_N$.\n",
    "In this way we can perfectly reconstruct the lowest $^1$/$_N$ th of our signals spectrum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Example:** We have an audio signal with a sampling rate of 32 kHz and hence an audio bandwidth of less than 16 kHz. For N=2 we low pass filter it to 8 kHz, to remove the frequencies at and above the new Nyquist frequency of 8 kHz. Then we can\n",
    "downsample it by a factor of N=2 (by dropping every second sample), to obtain an audio signal at a sampling rate of 16 kHz. \n",
    "\n",
    "We can the upsample the audio signal back to 32 kHz, by a factor of 2, by insterting a 0 after each sample, which produces “alias” components above 8 kHz, and then low pass filter the signal, again with our lowpass with cutoff frequency of 8 kHz, to remove the alias components. This results in the same audio signal with bandwidth of 8 kHz, but now at 32 kHz sampling rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Critical Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The following picture shows a **filter bank** with **critical sampling**, which means its downsampling rate N is identical to the number of subbands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Analysis Filter Bank\n",
    "It is the principal tool for multirate signal processing, first the analysis filter bank:\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/analysisFB.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Observe:** For each time step ***m*** we obtain a spectrum, and for each subband we obtain a narrow bandwidth time signal. So depending on our perspective, we have a set of spectra, or a set of narrow bandwidth time signal.\n",
    "### This is why we also call this a *time/frequency* representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Remember:** The **filter** boxes symbolize a **convolution** of the signal x (n) with the impulse response h(n) of length L of each filter, before downsampling:\n",
    "\n",
    "\\begin{equation}\\label{eq:1}\n",
    "\\Large\n",
    "x\\left(n \\right) * h\\left(n \\right) =  \\sum_{l=0}^{L-1}x\\left(n-l\\right) \\cdot h\\left(l\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where the sum is assumed to go over only the parts where x (n) and h(n) are defined. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Downsampling by N means we replace n by $mN+n_0$ , with m the index m at the lower sampling rate, and $n_0$ the phase\n",
    "index,\n",
    "\n",
    "\\begin{equation}\\label{eq:2}\n",
    "\\Large\n",
    "y_{n_0}^{\\uparrow N}\\left(m \\right) =  \\sum_{l=0}^{L-1}x\\left(mN+n_0-l\\right) \\cdot h_k\\left(l\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The analysis filter bank decomposes the signal into different frequency bands.\n",
    "\n",
    "**Observe:** Each frequency band has a lower sampling rate, which is possible because they have a lower bandwidth. <br>Using the “bandpass Nyquist” theorem we can reconstruct the original signal from the subbands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Synthesis Filter Bank\n",
    "To reconstruct the original from the different frequency bands, we need the **synthesis filter bank**:\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/synthesisFB.PNG\">\n",
    "</p>\n",
    "To simplify notation we dropped the downarrow and phase index for the subband signals $y_k(m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Observe:** The filters after the upsampling take on the role of the lowpass filter in the conventional Nyquist theorem, to block the alias components. <br> They “fish out” the correct frequency image out of the aliased images for that subband. All the subbands are then added up to reconstruct the original signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Example**: For our 2-band system: We have N=2, original sampling rate 32 kHz. Then the low pass branch corresponds to our low pass example above, which reconstructs the signal from 0 to 8 kHz. We now also have a high pass branch, which in addition reconstructs the frequency from 8kHz to 16 kHz. We add the two subbands in the synthesis filter bank to obtain the full bandwidth signal from 0 to 16 kHz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
